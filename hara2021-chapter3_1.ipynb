{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 「Python による経済・経営分析のための実践的データサイエンス」第二章 (pp.47-54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas の read_html を用いたデータのスクレイピングについて、本では取り上げていない事例も含めてご紹介したいと思います。\n",
    "まず、Pandas をインポートし、2020年の紅白歌合戦の出場歌手と売上をまとめたページの tableタグの情報をまとめて取得したいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                    歌手名                 曲名           発表年(売上)\n",
       " 0                 あいみょん                裸の心  2020(561,514・合算)\n",
       " 1                 石川さゆり           天城越え(12)      1986(48,500)\n",
       " 2                  坂本冬美       ブッダのように私は死んだ       2020(9,400)\n",
       " 3                  櫻坂46     Nobody's fault  2020(425,800・合算)\n",
       " 4                  JUJU        やさしさで溢れるように      2009(18,400)\n",
       " 5              Superfly        愛をこめて花束を(2)      2008(46,700)\n",
       " 6                 天童よしみ          あんたの花道(3)     2002(114,000)\n",
       " 7                  東京事変            うるうるうるう        2020(アルバム)\n",
       " 8                 NiziU     Make you happy          2020(配信)\n",
       " 9                 乃木坂46          Route 246          2020(配信)\n",
       " 10              Perfume             （メドレー）               NaN\n",
       " 11                日向坂46            アザトカワイイ        2020(アルバム）\n",
       " 12               Foorin            パプリカ(2)     2018(176,900)\n",
       " 13            BABYMETAL        イジメ、ダメ、ゼッタイ      2013(24,100)\n",
       " 14                 松田聖子          瑠璃色の地球(3)        1986(アルバム)\n",
       " 15                MISIA             アイノカタチ      2018(46,700)\n",
       " 16                水森かおり            瀬戸内 小豆島      2020(29,200)\n",
       " 17                milet         inside you      2019(15,000)\n",
       " 18                 LiSA             （メドレー）               NaN\n",
       " 19  Little Glee Monster                 足跡      2020(19,200)\n",
       " 20                  NaN                NaN               NaN\n",
       " 21                    嵐             （メドレー）               NaN\n",
       " 22                五木ひろし              山河(2)      2000(61,300)\n",
       " 23                   瑛人                 香水  2019(660,466・合算)\n",
       " 24       Official髭男dism          I LOVE...  2020(878,500・合算)\n",
       " 25                関ジャニ∞          前向きスクリーム！     2015(318,100)\n",
       " 26           Kis-My-Ft2  We never give up!     2011(329,000)\n",
       " 27        King & Prince          I promise         2020（未集計）\n",
       " 28                 郷ひろみ             （メドレー）               NaN\n",
       " 29          GENERATIONS            YOU & I          2020（配信）\n",
       " 30                   純烈             愛をください      2020(50,000)\n",
       " 31                 鈴木雅之             夢で逢えたら     1996(439,600)\n",
       " 32             SixTones     Imitation Rain   2020(1,760,900)\n",
       " 33             Snow Man               D.D.   2020(1,760,900)\n",
       " 34                氷川きよし      限界突破×サバイバー(2)      2017(21,900)\n",
       " 35                 福山雅治            家族になろうよ     2011(321,900)\n",
       " 36       Hey! Say! JUMP             （メドレー）               NaN\n",
       " 37                  星野源             うちで踊ろう          2020(配信)\n",
       " 38         Mr. Children   Documentary film        2020(アルバム)\n",
       " 39                三山ひろし             北のおんな町      2020(24,200)\n",
       " 40                 山内惠介              恋する街角       2008(9,000)\n",
       " 41                   ゆず         雨のち晴レルヤ(2)      2013(73,500)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# SSL証明書が正しくないサイトに対してPythonでアクセスする\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "#\n",
    "\n",
    "url1='https://www.wasteofpops.com/entry/2020/12/21/000000'\n",
    "dfs2= pd.read_html(url1)\n",
    "\n",
    "dfs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの正規化などは必要そうですが、出場歌手や曲名、売上や年の情報が取得できましたね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、本でも取り上げているパチンコホールの営業状況について read_html を用いたスクレイピングコードを実行してみましょう。輪転機がまわる本の出版プロセスの間に、どうやらサイト構造が変化したようです。そこで、1. 本に掲載したソースコード, 2. 2021年2月段階のサイト構造に即し修正したソースコードの両方を掲載します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 本に掲載したソースコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "list = [\"hokkaido\", \"aomori\",\"iwate\", \"miyagi\", \"akita\", \"yamagata\", \"fukushima\", \"ibaraki\", \"tochigi\", \"gunma\",\n",
    "        \"saitama\", \"chiba\", \"tokyo\", \"kanagawa\", \"niigata\", \"toyama\", \"ishikawa\",\"fukui\", \"yamanashi\", \"nagano\", \"gifu\",\n",
    "        \"shizuoka\", \"aichi\", \"mie\",\"shiga\", \"kyoto\", \"osaka\", \"hyogo\", \"nara\", \"wakayama\", \"tottori\", \"shimane\", \"okayama\",\n",
    "        \"hiroshima\", \"yamaguchi\", \"tokushima\", \"kagawa\", \"ehime\", \"kochi\", \"fukuoka\", \"saga\", \"nagasaki\", \"kumamoto\", \"oita\",\n",
    "       \"miyazaki\", \"kagoshima\", \"okinawa\"]\n",
    "\n",
    "#list = [\"tokyo\"]\n",
    "print(list)\n",
    "url1 = 'http://koko-pachi.com/htm/korona.' \n",
    "\n",
    "for j in range(len(list)):\n",
    "    data3=pd.DataFrame()\n",
    "    print(list[j])\n",
    "    url2 = list[j]+\".5.htm\"\n",
    "    url3=url1+url2\n",
    "    #print(url3)\n",
    "    dfs= pd.read_html(url3)\n",
    "    data3=data3.append(dfs)\n",
    "    \n",
    "    #data3\n",
    "    #print(data3)\n",
    "    #データ整理用\n",
    "    #簡易的にラベルを付与する\n",
    "    data3.columns={'A','B','C','D','E','F', 'G', 'H'}\n",
    "    \n",
    "    #不要な行を削除する\n",
    "    #data3=data3.dropna(how='all', axis=1)\n",
    "    #data3=data3.dropna(how='any')\n",
    "    data3=data3.drop('D', axis=1)\n",
    "    data3=data3.drop('E', axis=1)\n",
    "    data3=data3.drop('F', axis=1)\n",
    "    data3=data3.drop('G', axis=1)\n",
    "    data3=data3.drop('H', axis=1)\n",
    "    data3=data3.drop('B', axis=1)\n",
    "    data3=data3.drop('A', axis=1)\n",
    "    data3=data3.dropna(how='any')\n",
    "   \n",
    "\n",
    "    #データの整理. 余計な箇所を削除する\n",
    "    temp1=data3.loc[[0]]\n",
    "    temp1.reset_index(drop=True, inplace=True)\n",
    "    temp1.drop(temp1.tail(1).index, inplace=True)\n",
    "    temp1.drop(temp1.head(1).index, inplace=True)\n",
    "    temp1.drop(temp1.head(1).index, inplace=True)\n",
    "    temp1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #データの整理. index を並べ直して, データを取り出す.\n",
    "    temp2=data3.loc[[1]]\n",
    "    temp2.reset_index(drop=True, inplace=True)\n",
    "    temp3=data3.loc[[2]]\n",
    "    temp3.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    data5=pd.concat([temp1,temp2,temp3], axis=1)\n",
    "    \n",
    "    label=\"pachinko_\"+list[j]+\"_20200609.csv\"\n",
    "    data5.to_csv(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021年2月段階のサイト構造に即し修正したソースコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokyo']\n",
      "tokyo\n",
      "                                            A\n",
      "0                         ここって昔はパチンコ屋？(ここパチ？)\n",
      "0                                       再開712\n",
      "0                                          番号\n",
      "1                                           1\n",
      "2                                           2\n",
      "..                                        ...\n",
      "2   休業(2020/04/22〜2020/05/26)、再開(2020/05/27〜)\n",
      "0                              東京都葛飾区小菅4-10-8\n",
      "0                                  センチュリー 綾瀬店\n",
      "0                        東京都葛飾区亀有5-31-9マリトモビル\n",
      "0                              東京都葛飾区亀有5-31-9\n",
      "\n",
      "[144 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\murak\\AppData\\Local\\Temp\\ipykernel_15636\\3610321147.py:29: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data3=data3.append(dfs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "list = [\"hokkaido\", \"aomori\",\"iwate\", \"miyagi\", \"akita\", \"yamagata\", \"fukushima\", \"ibaraki\", \"tochigi\", \"gunma\",\n",
    "        \"saitama\", \"chiba\", \"tokyo\", \"kanagawa\", \"niigata\", \"toyama\", \"ishikawa\",\"fukui\", \"yamanashi\", \"nagano\", \"gifu\",\n",
    "        \"shizuoka\", \"aichi\", \"mie\",\"shiga\", \"kyoto\", \"osaka\", \"hyogo\", \"nara\", \"tottori\", \"shimane\", \"okayama\",\n",
    "        \"hiroshima\", \"yamaguchi\", \"tokushima\", \"kagawa\", \"ehime\", \"kochi\", \"fukuoka\", \"saga\", \"nagasaki\", \"kumamoto\", \"oita\",\n",
    "       \"miyazaki\", \"kagoshima\", \"okinawa\"]\n",
    "\n",
    "list = [\"tokyo\"]\n",
    "\n",
    "#どうやら和歌山だけサイト構造が他の都道府県と違うので、listから一時的に除外する\n",
    "#list2 = [\"wakayama\"]\n",
    "\n",
    "print(list)\n",
    "url1 = 'http://koko-pachi.com/htm/korona.' \n",
    "\n",
    "for j in range(len(list)):\n",
    "    data3=pd.DataFrame()\n",
    "    print(list[j])\n",
    "    #全体の営業状況のページが 5.htm から 6.htm に変更されているので修正\n",
    "    url2 = list[j]+\".6.htm\"\n",
    "    url3=url1+url2\n",
    "    #print(url3)\n",
    "    dfs= pd.read_html(url3)\n",
    "    print(dfs)\n",
    "    data3=data3.append(dfs)\n",
    "#    pd.concat([data3,dfs])\n",
    "    \n",
    "#    data3\n",
    "#    print(data3)\n",
    "    #データ整理用\n",
    "    #簡易的にラベルを付与する\n",
    "    data3.columns={'A','B','C','D','E','F', 'G'}\n",
    "    \n",
    "    #不要な行を削除する\n",
    "    # dropnaは NoN のある行を削除（axis=0は列、1は行）\n",
    "    #（allはすべてNoN、anyは１つでもNoNがあれば）\n",
    "    data3=data3.dropna(how='all', axis=1)\n",
    "    #data3=data3.dropna(how='any')\n",
    "    data3=data3.drop('D', axis=1)\n",
    "    data3=data3.drop('E', axis=1)\n",
    "    data3=data3.drop('F', axis=1)\n",
    "    data3=data3.drop('G', axis=1)\n",
    "    data3=data3.drop('C', axis=1)\n",
    "    #data3=data3.drop('H', axis=1)\n",
    "    data3=data3.drop('B', axis=1)\n",
    "    #data3=data3.drop('A', axis=1)\n",
    "    #data3=data3.dropna(how='any')\n",
    "    print(data3)\n",
    "\n",
    "    #データの整理. 余計な箇所を削除する\n",
    "    temp1=data3.loc[[0]]\n",
    "    temp1.reset_index(drop=True, inplace=True)\n",
    "    #temp1.drop(temp1.tail(1).index, inplace=True)\n",
    "    #temp1.drop(temp1.head(1).index, inplace=True)\n",
    "    #temp1.drop(temp1.head(1).index, inplace=True)\n",
    "    #temp1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #データの整理. index を並べ直して, データを取り出す.\n",
    "    temp2=data3.loc[[1]]\n",
    "    temp2.reset_index(drop=True, inplace=True)\n",
    "    temp3=data3.loc[[2]]\n",
    "    temp3.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    data5=pd.concat([temp1,temp2,temp3], axis=1)\n",
    "    #data5.reset_index(drop=True, inplace=True)\n",
    "    data5.columns=['住所','店名','状態']\n",
    "    data5=data5.dropna(how='any')\n",
    "    \n",
    "    label=\"pachinko_\"+list[j]+\"_20210215.csv\"\n",
    "    data5.to_csv(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本にも書きましたが、Web 空間上のサイトの構造はデザインや内容の変更に従い変化していきます。そのため、本に書いてあるスクレイピングの手法はあくまで一例として捉えて頂けると助かります。上記のソースコードもありますように、table タグをメインに構成されている Web サイトの場合には、Pandas の read_html を用いてまずはデータを取得し、Pandas のコードを組み合わせることで、自らが利用する情報にデータを整形することが必要不可欠な作業になるかと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献\n",
    "- pandas read_html; https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
